{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 An Extended Example: Tic-Tac-Toe\n",
    "\n",
    "### Setup\n",
    "\n",
    "<img src=\"tic_tac_toe.png\" align=\"left\" height=\"10\" width=\"150\" style=\"margin-right: 100px\">\n",
    "\n",
    "A imperfect player, who only makes random move, plays against a mostly greedy player, selecting a the move that leads to the state with greatest value.  \n",
    "Occasionally, however, the latter player selects moves that randomly. These are called _exploratory_ moves because they cause us to  experience states that he might otherwise never see.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"back_up.png\" height=\"10\" width=\"500\" align=\"right\" style=\"margin-right: 200px\">\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Here is an example of how tic-tac-toe would be approached with a method making use of a value function:\n",
    "1. We initialise a table of numbers, one for each state of the board. The state of the board is represented as a string by reading all positions row by row and concatenating them.\n",
    "   The value of the state is the probability of winning if the board is in that specific state.\n",
    "   E.g. {\"XXX O 0 0\": 0.5} \n",
    "2. We start playing against our opponent. Every time we need to make a move, we:  \n",
    "   2.1 List all the possible move  \n",
    "   2.2 Lookup the __state-value__ of each move from the __state-value__ dictionary  \n",
    "   2.3 Select the move with the highest __state-value__, i.e. the highest probability of winning  \n",
    "   2.4 We inform the previous state about the new probability of winning, updating its value using _temporal difference_ learning:\n",
    "   \\begin{equation}\n",
    "       V(S_t) \\leftarrow V(S_t) + \\alpha[V(S_{t+1}) - V(S_t)]\n",
    "   \\end{equation}\n",
    "   \n",
    "   \n",
    "The diagram on the left show as an example of backup update via temporal difference.  \n",
    "Note that the exploratory move (the one not marked with the `*`) does not result into value update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's setup the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The board is represented as a flat list of 9 positions, reading row by row, from left to right.\n",
    "An empty cell is represented by a whitespace `' '`.\n",
    "A scheme below:\n",
    "\n",
    "```\n",
    "[0, 1, 2]\n",
    "[3, 4, 5]\n",
    "[6, 7, 8]\n",
    "```\n",
    "\n",
    "Here is an example of a game at a random state, in which the two players use, `X`s and `O`s:\n",
    "```\n",
    "[' ', ' ', 'O']\n",
    "[' ', ' ', 'O']\n",
    "['X', 'X', 'X']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, player, opponent):\n",
    "        \"\"\"\n",
    "        Initialises a Tic-Tac-Toe environment, by specifying the two players in the game.\n",
    "        \"\"\"\n",
    "        self.board = [\" \" for x in range(9)]\n",
    "        self.player = player\n",
    "        self.opponent = opponent\n",
    "    \n",
    "        # init\n",
    "        self.player_turn = True\n",
    "        self.moves_left = 9\n",
    "        return\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state: an empty board\n",
    "        \"\"\"\n",
    "        self.board = [\" \" for x in range(9)]\n",
    "        self.moves_left = 9\n",
    "        return self.board, False\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs one play for the player that holds its turn.\n",
    "        This updates the board with the new move.\n",
    "        \"\"\"\n",
    "        if self.player_turn:\n",
    "            player = self.player\n",
    "        else:\n",
    "            player = self.opponent\n",
    "        \n",
    "        pos, reward = player.get_action(self.board)\n",
    "        self.board[pos] = player.marker\n",
    "        \n",
    "        self.player_turn = not self.player_turn\n",
    "        self.moves_left -= 1\n",
    "\n",
    "        return self.board, reward, self.game_ended()\n",
    "    \n",
    "    def learn(self, n_games):\n",
    "        \"\"\"\n",
    "        Plays `n_games` consecutively to let the player learn.\n",
    "        Args:\n",
    "            n_games (int): the number of games to learn from\n",
    "        \"\"\"\n",
    "        self.player.train()\n",
    "        for i in range(n_games):\n",
    "            print(\"Playing game {}\\t\".format(i), end=\"\\r\")\n",
    "            state, done = self.reset()\n",
    "            while not done:\n",
    "                state, reward, done = self.step()\n",
    "        return self.player\n",
    "    \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        Tests the skills of each player in the game by having a single game, whilst learning is disabled.\n",
    "        \"\"\"\n",
    "        self.player.eval()\n",
    "        state, done = self.reset()\n",
    "        while not done:\n",
    "            state, reward, done = self.step()\n",
    "            self.draw()\n",
    "        return state, done\n",
    "    \n",
    "    def game_ended(self):\n",
    "        \"\"\"\n",
    "        Checks if the game reached its conclusing, either because there are no moves left,\n",
    "        or because one of the two player has won.\n",
    "        \"\"\"\n",
    "        if self.moves_left <= 0:\n",
    "            return True\n",
    "        \n",
    "        state = self.board\n",
    "        # row\n",
    "        for i in range(0, 9, 3):\n",
    "            if (state[i] == state[i + 1] == state[i + 2]):\n",
    "                return state[i] != \" \"\n",
    "        # col\n",
    "        for i in range(3):\n",
    "            if (state[i] == state[i + 3] == state[i + 6]):\n",
    "                return state[i] != \" \"\n",
    "        # diag\n",
    "        if (state[0] == state[4] == state[8]):\n",
    "            return state[0] != \" \"\n",
    "        # anti-diag\n",
    "        if (state[2] == state[4] == state[6]):\n",
    "            return state[2] != \" \"\n",
    "        \n",
    "        return False        \n",
    "    \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Plots the current state of the board\n",
    "        \"\"\"\n",
    "        for i in range(0, 9, 3):\n",
    "            print(self.board[i:i + 3])\n",
    "        print(\"State value:\", self.player.get_value(self.board))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's setup an imperfect player, who can only make random moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import numpy\n",
    "\n",
    "\n",
    "class RandomPlayer:\n",
    "    def __init__(self, marker):\n",
    "        self.marker = str(marker)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_action(state):\n",
    "        \"\"\"\n",
    "        Given the current state of the board, selects a random move from all the available moves\n",
    "        \"\"\"\n",
    "        pos = random.randint(0, 8)\n",
    "        while state[pos] != \" \":\n",
    "            pos = random.randint(0, 8)\n",
    "        return pos, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And an E-Greedy player\n",
    "\n",
    "The e-greedy player makes greedy moves, i.e. moves whose value is max among the list of all possibile moves.  \n",
    "Occasionally, with probability `e`, it makes an _exploratory_ move, a move that would not experience otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyPlayer:\n",
    "    def __init__(self, marker, init_value=0.5, e_greedy=0., step_size=0.5,\n",
    "                 decrement=0.9,\n",
    "                 decrement_each=1000,\n",
    "                 learn=True):\n",
    "        self.marker = str(marker)\n",
    "        self.learn = learn\n",
    "        self.e_greedy = e_greedy\n",
    "        self.step_size = step_size\n",
    "        self.init_value = init_value\n",
    "        self.decrement = decrement\n",
    "        self.decrement_each = decrement_each\n",
    "        \n",
    "        # we store each state of the board as a dict whose key is the hash of the list\n",
    "        # and the value is its state value\n",
    "        self.state_value = {}\n",
    "        self.n_plays = 0\n",
    "        return\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Enables updates of the state-value table while playing\n",
    "        \"\"\"\n",
    "        self.learn = True\n",
    "        \n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Disables updates of the state-value table while playing\n",
    "        \"\"\"\n",
    "        self.learn = False\n",
    "    \n",
    "    def encode(self, state):\n",
    "        \"\"\"\n",
    "        Encodes the state of the board into a single string\n",
    "        Args:\n",
    "            state (List[str]): The board state as a list of strings: the player markers\n",
    "        \"\"\"\n",
    "        return \"\".join(state)\n",
    "    \n",
    "    def set_value(self, state, value):\n",
    "        \"\"\"\n",
    "        Updates the state-value of the given state in the table\n",
    "        Args:\n",
    "            state (List[str]): The board state as a list of strings: the player markers\n",
    "            value (float): the probability of winning in that state\n",
    "        \"\"\"\n",
    "        encoded = self.encode(state)\n",
    "        self.state_value[encoded] = value\n",
    "        return \n",
    "    \n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Performs a lookup to the state-value table for the given state.\n",
    "        Args:\n",
    "            state (List[str]): The board state as a list of strings: the player markers\n",
    "        Returns:\n",
    "            (float): the value of the state, if already visited, the initial value otherwise\n",
    "        \n",
    "        \"\"\"\n",
    "        encoded = self.encode(state)\n",
    "        if encoded in self.state_value:\n",
    "            return self.state_value[encoded]\n",
    "        else:\n",
    "            return self.init_value\n",
    "    \n",
    "    def back_up(self, state, next_state, next_value):\n",
    "        \"\"\"\n",
    "        Performs a backup update of the current state using temporal difference learning\n",
    "        Args:\n",
    "            state (List[str]): The current state of the board as a list of strings: the player markers\n",
    "            next_state (List[str]): The state of the board if we had to make this move, as a list of strings\n",
    "            next_value (float): the value of the next state, if we had to make this move\n",
    "        \"\"\"\n",
    "        current_value = self.get_value(state)           \n",
    "        self.set_value(next_state, next_value)\n",
    "\n",
    "        # TD update\n",
    "        new_value = current_value + self.step_size * (next_value - current_value)\n",
    "        self.set_value(state, new_value)\n",
    "        return\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Computes the e-greedy action, according to the current state of the board.\n",
    "        Args:\n",
    "            state (List[str]): The current state of the board\n",
    "        Returns:\n",
    "            (Tuple[int, float]): A tuple containing the best move and the reward deriving from it\n",
    "        \"\"\"\n",
    "        # perform exploratory move with probability self.e_greedy\n",
    "        # do not update value table\n",
    "        if random.random() < self.e_greedy and self.learn:\n",
    "            return RandomPlayer.get_action(state)\n",
    "        \n",
    "        best_move = None\n",
    "        best_state = None\n",
    "        best_reward = -float(\"inf\")\n",
    "        for pos in range(len(state)):\n",
    "            # check that the cell is free\n",
    "            if state[pos] == \" \":    \n",
    "                next_state = copy.deepcopy(state)\n",
    "                next_state[pos] = self.marker\n",
    "                reward = self.get_reward(next_state)\n",
    "\n",
    "                # choose greedily\n",
    "                if reward >= best_reward:\n",
    "                    best_move = pos\n",
    "                    best_state = next_state\n",
    "                    best_reward = reward\n",
    "        \n",
    "        if self.learn:\n",
    "            self.back_up(state, best_state, best_reward)\n",
    "        \n",
    "        self.n_plays += 1\n",
    "        if self.n_plays % self.decrement_each == 0:\n",
    "            self.step_size *= self.decrement\n",
    "        \n",
    "        return best_move, best_reward\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        \"\"\"\n",
    "        Computes the reward of the player at a given state of the board\n",
    "        Args:\n",
    "            state (List[str]): The current state of the board\n",
    "        Returns:\n",
    "            (float): 1 if the move is a winning move, 0 is the state is making the player lose.\n",
    "                     In all the other cases returns the probability of the current state as stored in the \n",
    "                     state-value table.\n",
    "        \"\"\"\n",
    "        # row\n",
    "        for i in range(0, 9, 3):\n",
    "            if (state[i] == state[i + 1] == state[i + 2]):\n",
    "                # three in a row\n",
    "                if state[i] == self.marker:\n",
    "                    return 1  # won, win prop 1.\n",
    "                elif state[i] == \" \":\n",
    "                    return self.get_value(state)  # row is empty\n",
    "                else:\n",
    "                    return 0  # lost, win prob 0.       \n",
    "        # col\n",
    "        for i in range(3):\n",
    "            if (state[i] == state[i + 3] == state[i + 6]):\n",
    "                # three in a row\n",
    "                if state[i] == self.marker:\n",
    "                    return 1  # won, win prop 1.\n",
    "                elif state[i] == \" \":\n",
    "                    return self.get_value(state)  # row is empty\n",
    "                else:\n",
    "                    return 0  # lost, win prob 0.                \n",
    "        # diag\n",
    "        if (state[0] == state[4] == state[8]):\n",
    "            if not self.learn:\n",
    "                print(\"diag\")\n",
    "            # three in a row\n",
    "            if state[0] == self.marker:\n",
    "                return 1  # won, win prop 1.\n",
    "            elif state[0] == \" \":\n",
    "                return self.get_value(state)  # row is empty\n",
    "            else:\n",
    "                return 0  # lost, win prob 0.\n",
    "        # anti-diag\n",
    "        if (state[2] == state[4] == state[6]):\n",
    "            # three in a row\n",
    "            if state[2] == self.marker:\n",
    "                return 1  # won, win prop 1.\n",
    "            elif state[2] == \" \":\n",
    "                return self.get_value(state)  # row is empty\n",
    "            else:\n",
    "                return 0  # lost, win prob 0.\n",
    "        return self.get_value(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "opponent = RandomPlayer(\"O\")\n",
    "player = EGreedyPlayer(\"X\", init_value=0.5, e_greedy=0.3, step_size=0.5, decrement=0.8, decrement_each=10000)\n",
    "env = TicTacToe(opponent=opponent, player=player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing game 99999\t\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EGreedyPlayer at 0x7f69b725c6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.learn(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'O', ' ']\n",
      "[' ', ' ', ' ']\n",
      "[' ', ' ', ' ']\n",
      "State value: 0.5\n",
      "\n",
      "[' ', 'O', ' ']\n",
      "[' ', ' ', ' ']\n",
      "[' ', ' ', 'X']\n",
      "State value: 0.5\n",
      "\n",
      "[' ', 'O', 'O']\n",
      "[' ', ' ', ' ']\n",
      "[' ', ' ', 'X']\n",
      "State value: 0.999166204770979\n",
      "\n",
      "['X', 'O', 'O']\n",
      "[' ', ' ', ' ']\n",
      "[' ', ' ', 'X']\n",
      "State value: 0.999197888318448\n",
      "\n",
      "['X', 'O', 'O']\n",
      "[' ', ' ', ' ']\n",
      "[' ', 'O', 'X']\n",
      "State value: 0.9999999827065554\n",
      "\n",
      "diag\n",
      "['X', 'O', 'O']\n",
      "[' ', 'X', ' ']\n",
      "[' ', 'O', 'X']\n",
      "State value: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['X', 'O', 'O', ' ', 'X', ' ', ' ', 'O', 'X'], True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002951479051793532"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player.step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
